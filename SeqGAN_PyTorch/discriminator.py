# -*- coding: utf-8 -*-

import os
import random

import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

class Discriminator(nn.Module):
    """A CNN for text classification

<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> d083f639ce45b5e56f8fe2ed8676259f23ca06f1
    architecture: Embedding >> Convolution >> ReLU >> Max-pooling >> Highway >> Softmax (Real/Fake & Class)
    """

    def __init__(self, sequence_length, num_classes, vocab_size, emb_dim, filter_sizes, 
                 num_filters, dropout, l2_reg_lambda=1.0, wgan_reg_lambda=1.0, grad_clip=1.0):
        super(Discriminator, self).__init__()

        self.sequence_length = sequence_length
        self.num_classes = num_classes
        self.vocab_size = vocab_size
        self.emb_dim = emb_dim
        self.filter_sizes = filter_sizes
        self.num_filters = num_filters
        self.dropout = dropout
        self.l2_reg_lambda = l2_reg_lambda
        self.wgan_reg_lambda = wgan_reg_lambda
        self.grad_clip = grad_clip

        self.emb = nn.Embedding(vocab_size, emb_dim)
        self.class_emb = nn.Embedding(num_classes, emb_dim) # Class embedding

        self.convs = nn.ModuleList([
            nn.Conv2d(1, num_filter, (filter_size, emb_dim))
            for filter_size, num_filter in zip(filter_sizes, num_filters)
        ])

        self.highway = self.highway_fn
<<<<<<< HEAD
=======
=======
    architecture: Embedding >> Convolution >> Max-pooling >> Highway >> Softmax (Real/Fake & Class)
    """

    def __init__(self, num_classes, vocab_size, emb_dim, filter_sizes, num_filters, dropout):
        super(Discriminator, self).__init__()
        self.emb = nn.Embedding(vocab_size, emb_dim)
        self.convs = nn.ModuleList([
            nn.Conv2d(1, n, (f, emb_dim)) for (n, f) in zip(num_filters, filter_sizes)
        ])
        self.highway = nn.Linear(sum(num_filters), sum(num_filters))
>>>>>>> b594693170c35e64bbff3b586c9d8a6b2a38966b
>>>>>>> d083f639ce45b5e56f8fe2ed8676259f23ca06f1
        self.dropout = nn.Dropout(p=dropout)

        # Linear layers for Real/Fake classification and Class classification
        self.lin_real_fake = nn.Linear(sum(num_filters), 1)  # Binary output for real/fake
        self.lin_class = nn.Linear(sum(num_filters), num_classes)  # Class output
        # self.lin = nn.Linear(sum(num_filters), num_classes)
        # self.softmax = nn.LogSoftmax()

<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> d083f639ce45b5e56f8fe2ed8676259f23ca06f1
        # self.init_parameters()

    def forward(self, x, class_label):
        """
        Args:
            x: (batch_size, seq_len), sequence of tokens generated by generator
            class_label: (batch_size, ), class label for the sequences
        """
        emb = self.emb(x) # (batch_size, seq_len, emb_dim)

        # Embedding for class labels and expand to match sequence length
        class_emb = self.class_emb(class_label).unsqueeze(1)  # (batch_size, 1, emb_dim)
        class_emb = class_emb.expand(-1, x.size(1), -1)       # (batch_size, seq_len, emb_dim)
        # Concatenate token embeddings with class embeddings
        combined_emb = torch.cat([emb, class_emb], dim=-1)  # (batch_size, seq_len, emb_dim * 2)
        combined_emb = combined_emb.unsqueeze(1)  # Add channel dimension: (batch_size, 1, seq_len, emb_dim * 2)
           
        # convs = [F.relu(conv(combined_emb)).squeeze(3) for conv in self.convs]  # [batch_size * num_filter * length]
        # pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs] # [batch_size * num_filter]
        # pred = torch.cat(pools, 1)  # batch_size * num_filters_sum
        
        pooled_outputs = []
        for conv in self.convs:
            conv_out = F.relu(conv(combined_emb)) 
            pooled = F.max_pool2d(conv_out, (conv_out.size(2), 1)) 
            pooled_outputs.append(pooled.squeeze(3))  

        # Concatenate the pooled outputs and flatten
        h_pool = torch.cat(pooled_outputs, 1)
        h_pool_flat = h_pool.view(-1, h_pool.size(1))

        highway = self.highway(h_pool, h_pool_flat.size(1))  # Apply highway
        pred = self.dropout(highway) # dropout

<<<<<<< HEAD
=======
=======
        self.init_parameters()

    def forward(self, x):
        """
        Args:
            x: (batch_size * seq_len)
        """
        emb = self.emb(x).unsqueeze(1)  # batch_size * 1 * seq_len * emb_dim
        convs = [F.relu(conv(emb)).squeeze(3) for conv in self.convs]  # [batch_size * num_filter * length]
        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs] # [batch_size * num_filter]
        pred = torch.cat(pools, 1)  # batch_size * num_filters_sum
        highway = self.highway(pred)
        pred = torch.sigmoid(highway) *  F.relu(highway) + (1. - torch.sigmoid(highway)) * pred
        # Apply dropout
        pred = self.dropout(pred)
>>>>>>> b594693170c35e64bbff3b586c9d8a6b2a38966b
>>>>>>> d083f639ce45b5e56f8fe2ed8676259f23ca06f1
        # Real/Fake classification
        real_fake_output = torch.sigmoid(self.lin_real_fake(pred))  # Binary classification (real/fake)
        # Class prediction
        class_output = self.lin_class(pred)  # Class classification (multiclass)
        return real_fake_output, class_output

<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> d083f639ce45b5e56f8fe2ed8676259f23ca06f1
    # def init_parameters(self):
    #     for param in self.parameters():
    #         param.data.uniform_(-0.05, 0.05)


    def highway_fn(self, input_, size, num_layers=1, bias=-2.0, f=F.relu):
        """
        Highway Network (cf. http://arxiv.org/abs/1505.00387).
        t = sigmoid(Wy + b)
        z = t * g(Wy + b) + (1 - t) * y
        where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.
        """
        for idx in range(num_layers):
            g = f(self.linear_fn(input_, size))

            t = torch.sigmoid(self.linear_fn(input_, size) + bias)

            output = t * g + (1. - t) * input_
            input_ = output

        return output

    def linear_fn(input_, output_size):
        '''
        Linear map: output[k] = sum_i(Matrix[k, i] * input_[i] ) + Bias[k]
        Args:
        input_: a tensor or a list of 2D, batch x n, Tensors.
        output_size: int, second dimension of W[i].
    Returns:
        A 2D Tensor with shape [batch x output_size] equal to
        sum_i(input_[i] * W[i]), where W[i]s are newly created matrices.
    Raises:
        ValueError: if some of the arguments has unspecified or wrong shape.
    '''

        shape = list(input_.size())
        # print(shape)
        if len(shape) != 2:
            raise ValueError("Linear is expecting 2D arguments: %s" % str(shape))
        if not shape[1]:
            raise ValueError(
                "Linear expects shape[1] of arguments: %s" % str(shape))
        input_size = shape[1]

        linear_layer = nn.Linear(input_size, output_size)

        return linear_layer(input_)
<<<<<<< HEAD
=======
=======
    def init_parameters(self):
        for param in self.parameters():
            param.data.uniform_(-0.05, 0.05)
>>>>>>> b594693170c35e64bbff3b586c9d8a6b2a38966b
>>>>>>> d083f639ce45b5e56f8fe2ed8676259f23ca06f1
