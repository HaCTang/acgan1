# -*- coding: utf-8 -*-

import os
import random

import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

class Discriminator(nn.Module):
    """A CNN for text classification

    architecture: Embedding >> Convolution >> ReLU >> Max-pooling >> Highway >> Softmax (Real/Fake & Class)
    """

    def __init__(self, sequence_length, num_classes, vocab_size, emb_dim, filter_sizes, use_cuda,
                 num_filters, dropout=0.75, l2_reg_lambda=0.001, wgan_reg_lambda=1.0, grad_clip=1.0):
        super(Discriminator, self).__init__()

        self.sequence_length = sequence_length
        self.num_classes = num_classes
        self.vocab_size = vocab_size
        self.emb_dim = emb_dim
        self.filter_sizes = filter_sizes
        self.use_cuda = use_cuda
        self.num_filters = num_filters
        self.dropout = dropout
        self.l2_reg_lambda = l2_reg_lambda
        self.wgan_reg_lambda = wgan_reg_lambda
        self.grad_clip = grad_clip

        self.emb = nn.Embedding(vocab_size, emb_dim)

        #self.d_count = 0 # for reporting

        self.convs = nn.ModuleList([
            nn.Conv2d(1, num_filter, (filter_size, emb_dim))
            for filter_size, num_filter in zip(filter_sizes, num_filters)
        ])

        self.highway_layer = nn.Linear(sum(num_filters), sum(num_filters))
        self.dropout_layer = nn.Dropout(p=dropout)

        # Linear layers for Real/Fake classification and Class classification
        self.lin_real_fake = nn.Linear(sum(num_filters), 2)  # Binary output for real/fake
        self.classifier = nn.Linear(sum(num_filters), num_classes)  # Class output

        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001)
        self.adversarial_loss = nn.BCELoss()
        self.auxiliary_loss = F.cross_entropy

        if self.use_cuda:
            self.to(torch.device("cuda"))

        nn.init.uniform_(self.emb.weight, -1.0, 1.0)
        nn.init.trunc_normal_(self.lin_real_fake.weight, std=0.1)
        nn.init.constant_(self.lin_real_fake.bias, 0.1)

    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len), sequence of tokens generated by generator
        """
        if self.use_cuda:
            x = x.to(torch.device("cuda"))
        
        emb = self.emb(x).unsqueeze(1) # (batch_size, 1, seq_len, emb_dim)
           
        pooled_outputs = []
        for conv in self.convs:
            conv_out = F.relu(conv(emb)).squeeze(3) # (batch_size, num_filters, seq_len)
            pooled = F.max_pool1d(conv_out, (conv_out.size(2)))
            pooled_outputs.append(pooled.squeeze(2)) # (batch_size, num_filters)

        # Concatenate the pooled outputs and flatten
        h_pool = torch.cat(pooled_outputs, dim=1)
        h_pool_flat = h_pool.flatten(start_dim=1)

        # Apply highway
        h = self.highway_layer(h_pool_flat)
        t = torch.sigmoid(h)
        highway = t * F.relu(h) + (1.0 - t) * h_pool_flat

        pred = self.dropout_layer(highway) # dropout
        real_fake_output = self.lin_real_fake(pred)
        # 直接返回logits,不应用softmax
        return real_fake_output, None

    def compute_loss(self, real_fake_pred, real_fake_target, class_pred, class_target):
        """
        Discriminator loss combines real/fake classification loss and class prediction loss.
        
        Args:
            real_fake_pred: (N, 1) - Predictions for real/fake classification
            real_fake_target: (N, 1) - Ground truth for real/fake classification
            class_pred: (N, num_classes) - Predictions for class classification
            class_target: (N, ) - Ground truth class labels
        """
        # 分离正负样本
        pos_mask = (real_fake_target[:, 1] == 1)
        neg_mask = (real_fake_target[:, 0] == 1)
        
        scores_pos = real_fake_pred[pos_mask]
        scores_neg = real_fake_pred[neg_mask]

        # 计算Wasserstein损失
        wgan_loss = torch.abs(
            scores_neg.mean() - scores_pos.mean()
        )
        wgan_loss = self.wgan_reg_lambda * wgan_loss

        # L2正则化
        l2_loss = 0
        for param in self.parameters():
            l2_loss += torch.norm(param, p=2)
        l2_loss = self.l2_reg_lambda * l2_loss
        
        total_loss = l2_loss + wgan_loss
        return total_loss

    def train_step(self, x, real_fake_labels, class_labels, dropout=0.2):
        """
        Args:
            x: (batch_size, seq_len), sequence of tokens generated by generator
            real_fake_labels: (batch_size, 1) - Ground truth for real/fake classification: [1.] or [0.]
            class_labels: (batch_size, ) - Ground truth class labels
        """
        if self.use_cuda:
            x = x.to(torch.device("cuda"))
            real_fake_labels = real_fake_labels.to(torch.device("cuda"))
            class_labels = class_labels.to(torch.device("cuda"))
        
        self.dropout = dropout
        self.train()
        self.optimizer.zero_grad()

        # Forward pass
        real_fake_output, class_output = self.forward(x)

        # Calculate loss
        loss = self.compute_loss(real_fake_output, real_fake_labels, class_output, class_labels)

        # Backward pass
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.parameters(), self.grad_clip)
        self.optimizer.step()

        return loss.item()

if __name__ == "__main__":
    # Test code for Discriminator

    # Set random seed for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)

    # Parameters
    sequence_length = 10
    num_classes = 2
    vocab_size = 5000
    emb_dim = 128
    filter_sizes = [3, 4, 5]
    use_cuda = True
    num_filters = [100, 100, 100]
    dropout = 0.5

    # Create discriminator model
    discriminator = Discriminator(sequence_length, num_classes, vocab_size, emb_dim, filter_sizes, use_cuda, num_filters, dropout)

    # Real data (batch_size, seq_len)
    real_data = torch.randint(0, vocab_size, (32, sequence_length))
    real_class_labels = torch.randint(0, num_classes, (32,))
    real_labels = torch.ones((32, 1), dtype=torch.float)  # Real data labels (all 1s)

    # Fake data (batch_size, seq_len)
    fake_data = torch.randint(0, vocab_size, (32, sequence_length))
    fake_class_labels = torch.randint(0, num_classes, (32,))
    fake_labels = torch.zeros((32, 1), dtype=torch.float)  # Fake data labels (all 0s)

    # Forward pass for real data
    real_output, real_class_output = discriminator(real_data)
    real_loss = discriminator.compute_loss(real_output, real_labels, real_class_output, real_class_labels)

    # Forward pass for fake data
    fake_output, fake_class_output = discriminator(fake_data)
    fake_loss = discriminator.compute_loss(fake_output, fake_labels, fake_class_output, fake_class_labels)

    # Total loss
    total_loss = 0.5 * (real_loss + fake_loss)

    print(f"Real Loss: {real_loss.item():.4f}")
    print(f"Fake Loss: {fake_loss.item():.4f}")
    print(f"Total Loss: {total_loss.item():.4f}")