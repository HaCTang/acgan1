# -*- coding: utf-8 -*-

import os
import random

import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

class Discriminator(nn.Module):
    """A CNN for text classification

    architecture: Embedding >> Convolution >> ReLU >> Max-pooling >> Highway >> Softmax (Real/Fake & Class)
    """

    def __init__(self, sequence_length, num_classes, vocab_size, emb_dim, filter_sizes, 
                 num_filters, dropout, l2_reg_lambda=1.0, wgan_reg_lambda=1.0, grad_clip=1.0):
        super(Discriminator, self).__init__()

        self.sequence_length = sequence_length
        self.num_classes = num_classes
        self.vocab_size = vocab_size
        self.emb_dim = emb_dim
        self.filter_sizes = filter_sizes
        self.num_filters = num_filters
        self.dropout = dropout
        self.l2_reg_lambda = l2_reg_lambda
        self.wgan_reg_lambda = wgan_reg_lambda
        self.grad_clip = grad_clip

        self.emb = nn.Embedding(vocab_size, emb_dim)
        self.class_emb = nn.Embedding(num_classes, emb_dim) # Class embedding

        self.convs = nn.ModuleList([
            nn.Conv2d(1, num_filter, (filter_size, emb_dim))
            for filter_size, num_filter in zip(filter_sizes, num_filters)
        ])

        self.highway = self.highway_fn
        self.dropout = nn.Dropout(p=dropout)

        # Linear layers for Real/Fake classification and Class classification
        self.lin_real_fake = nn.Linear(sum(num_filters), 1)  # Binary output for real/fake
        self.lin_class = nn.Linear(sum(num_filters), num_classes)  # Class output
        # self.lin = nn.Linear(sum(num_filters), num_classes)
        # self.softmax = nn.LogSoftmax()

        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001)

        # self.init_parameters()

    def forward(self, x, class_label):
        """
        Args:
            x: (batch_size, seq_len), sequence of tokens generated by generator
            class_label: (batch_size, ), class label for the sequences
        """
        emb = self.emb(x) # (batch_size, seq_len, emb_dim)

        # Embedding for class labels and expand to match sequence length
        class_emb = self.class_emb(class_label).unsqueeze(1)  # (batch_size, 1, emb_dim)
        class_emb = class_emb.expand(-1, x.size(1), -1)       # (batch_size, seq_len, emb_dim)
        # Concatenate token embeddings with class embeddings
        combined_emb = torch.cat([emb, class_emb], dim=-1)  # (batch_size, seq_len, emb_dim * 2)
        combined_emb = combined_emb.unsqueeze(1)  # Add channel dimension: (batch_size, 1, seq_len, emb_dim * 2)
           
        # convs = [F.relu(conv(combined_emb)).squeeze(3) for conv in self.convs]  # [batch_size * num_filter * length]
        # pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs] # [batch_size * num_filter]
        # pred = torch.cat(pools, 1)  # batch_size * num_filters_sum
        
        pooled_outputs = []
        for conv in self.convs:
            conv_out = F.relu(conv(combined_emb)) 
            pooled = F.max_pool2d(conv_out, (conv_out.size(2), 1)) 
            pooled_outputs.append(pooled.squeeze(3))  

        # Concatenate the pooled outputs and flatten
        h_pool = torch.cat(pooled_outputs, 1)
        h_pool_flat = h_pool.view(-1, h_pool.size(1))

        highway = self.highway(h_pool, h_pool_flat.size(1))  # Apply highway
        pred = self.dropout(highway) # dropout

        # Real/Fake classification
        real_fake_output = torch.sigmoid(self.lin_real_fake(pred))  # Binary classification (real/fake)
        # Class prediction
        class_output = self.lin_class(pred)  # Class classification (multiclass)
        return real_fake_output, class_output

    # def init_parameters(self):
    #     for param in self.parameters():
    #         param.data.uniform_(-0.05, 0.05)

    def compute_loss(self, real_fake_pred, real_fake_target, class_pred, class_target):
        """
        Discriminator loss combines real/fake classification loss and class prediction loss.
        
        Args:
            real_fake_pred: (N, 1) - Predictions for real/fake classification
            real_fake_target: (N, 1) - Ground truth for real/fake classification
            class_pred: (N, num_classes) - Predictions for class classification
            class_target: (N, ) - Ground truth class labels
        """
        real_fake_loss = F.cross_entropy(real_fake_pred, real_fake_target)
        class_loss = F.cross_entropy(class_pred, class_target)
        
        l2_loss = 0
        for param in self.parameters():
            l2_loss += torch.norm(param, p=2)
        l2_loss = self.l2_reg_lambda * l2_loss

        # # Wasserstein loss (simplified, adjust according to the exact TF implementation)
        # scores_neg = real_fake_pred[real_fake_target == 0]  # Fake scores
        # scores_pos = real_fake_pred[real_fake_target == 1]  # Real scores
        # wgan_loss = torch.abs(torch.mean(scores_neg) - torch.mean(scores_pos))
        # wgan_loss = self.wgan_reg_lambda * wgan_loss
        
        total_loss = real_fake_loss + class_loss + l2_loss      
        return total_loss

    def train_step(self, x, class_label, real_fake_labels, class_labels, optimizer):
        self.train()
        self.optimizer.zero_grad()

        # Forward pass
        real_fake_output, class_output = self.forward(x, class_label)

        # Calculate loss
        loss = self.compute_loss(real_fake_output, class_output, real_fake_labels, class_labels)

        # Backward pass
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.parameters(), self.grad_clip)
        self.optimizer.step()

        return loss.item() 

    def highway_fn(self, input_, size, num_layers=1, bias=-2.0, f=F.relu):
        """
        Highway Network (cf. http://arxiv.org/abs/1505.00387).
        t = sigmoid(Wy + b)
        z = t * g(Wy + b) + (1 - t) * y
        where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.
        """
        for idx in range(num_layers):
            g = f(self.linear_fn(input_, size))

            t = torch.sigmoid(self.linear_fn(input_, size) + bias)

            output = t * g + (1. - t) * input_
            input_ = output

        return output

    def linear_fn(input_, output_size):
        '''
        Linear map: output[k] = sum_i(Matrix[k, i] * input_[i] ) + Bias[k]
        Args:
        input_: a tensor or a list of 2D, batch x n, Tensors.
        output_size: int, second dimension of W[i].
    Returns:
        A 2D Tensor with shape [batch x output_size] equal to
        sum_i(input_[i] * W[i]), where W[i]s are newly created matrices.
    Raises:
        ValueError: if some of the arguments has unspecified or wrong shape.
    '''

        shape = list(input_.size())
        # print(shape)
        if len(shape) != 2:
            raise ValueError("Linear is expecting 2D arguments: %s" % str(shape))
        if not shape[1]:
            raise ValueError(
                "Linear expects shape[1] of arguments: %s" % str(shape))
        input_size = shape[1]

        linear_layer = nn.Linear(input_size, output_size)

        return linear_layer(input_)
