# -*- coding: utf-8 -*-

import os
import random

import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

class Discriminator(nn.Module):
    """A CNN for text classification

    architecture: Embedding >> Convolution >> ReLU >> Max-pooling >> Highway >> Softmax (Real/Fake & Class)
    """

    def __init__(self, sequence_length, num_classes, vocab_size, emb_dim, filter_sizes, 
                 num_filters, dropout, l2_reg_lambda=1.0, wgan_reg_lambda=1.0, grad_clip=1.0):
        super(Discriminator, self).__init__()

        self.sequence_length = sequence_length
        self.num_classes = num_classes
        self.vocab_size = vocab_size
        self.emb_dim = emb_dim
        self.filter_sizes = filter_sizes
        self.num_filters = num_filters
        self.dropout = dropout
        self.l2_reg_lambda = l2_reg_lambda
        self.wgan_reg_lambda = wgan_reg_lambda
        self.grad_clip = grad_clip

        self.emb = nn.Embedding(vocab_size, emb_dim)

        self.convs = nn.ModuleList([
            nn.Conv2d(1, num_filter, (filter_size, emb_dim))
            for filter_size, num_filter in zip(filter_sizes, num_filters)
        ])

        self.highway_transform = nn.Linear(sum(num_filters), sum(num_filters))
        self.highway_gate = nn.Linear(sum(num_filters), sum(num_filters))
        self.dropout_layer = nn.Dropout(p=dropout)

        # Linear layers for Real/Fake classification and Class classification
        self.lin_real_fake = nn.Linear(sum(num_filters), 1)  # Binary output for real/fake
        self.classifier = nn.Linear(sum(num_filters), num_classes)  # Class output

        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001)
        self.adversarial_loss = F.binary_cross_entropy
        self.auxiliary_loss = F.cross_entropy


    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len), sequence of tokens generated by generator
        """
        emb = self.emb(x).unsqueeze(1) # (batch_size, 1, seq_len, emb_dim)
           
        pooled_outputs = []
        for conv in self.convs:
            conv_out = F.relu(conv(emb))
            pooled = F.max_pool2d(conv_out, (conv_out.size(2), 1))
            pooled_outputs.append(pooled.squeeze(3))

        # Concatenate the pooled outputs and flatten
        h_pool = torch.cat(pooled_outputs, 1)
        h_pool_flat = h_pool.flatten(start_dim=1)

        # Apply highway
        transform = F.relu(self.highway_transform(h_pool_flat))
        gate = torch.sigmoid(self.highway_gate(h_pool_flat))
        highway = gate * transform + (1.0 - gate) * h_pool_flat

        pred = self.dropout_layer(highway) # dropout

        # Real/Fake classification
        real_fake_output = torch.sigmoid(self.lin_real_fake(pred))  # Binary classification (real/fake)
        # Class prediction
        class_output = self.classifier(pred)  # Class classification (multiclass)
        return real_fake_output, class_output

    def compute_loss(self, real_fake_pred, real_fake_target, class_pred, class_target):
        """
        Discriminator loss combines real/fake classification loss and class prediction loss.
        
        Args:
            real_fake_pred: (N, 1) - Predictions for real/fake classification
            real_fake_target: (N, 1) - Ground truth for real/fake classification
            class_pred: (N, num_classes) - Predictions for class classification
            class_target: (N, ) - Ground truth class labels
        """
        real_fake_loss = self.adversarial_loss(real_fake_pred, real_fake_target)
        class_loss = self.auxiliary_loss(class_pred, class_target)
        
        l2_loss = 0
        for param in self.parameters():
            l2_loss += torch.norm(param, p=2)
        l2_loss = self.l2_reg_lambda * l2_loss

        # # Wasserstein loss (simplified, adjust according to the exact TF implementation)
        # scores_neg = scores[input_y == 0]
        # scores_pos = scores[input_y == 1]
        # wgan_loss = torch.abs(torch.mean(scores_neg) - torch.mean(scores_pos))
        # wgan_loss = self.wgan_reg_lambda * wgan_loss

        total_loss = 0.5 * (real_fake_loss + class_loss) + l2_loss      
        return total_loss

    def train_step(self, x, class_label, real_fake_labels, class_labels):
        self.train()
        self.optimizer.zero_grad()

        # Forward pass
        real_fake_output, class_output = self.forward(x)

        # Calculate loss
        loss = self.compute_loss(real_fake_output, real_fake_labels, class_output, class_labels)

        # Backward pass
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.parameters(), self.grad_clip)
        self.optimizer.step()

        return loss.item() 

if __name__ == "__main__":
    # Test code for Discriminator

    # Set random seed for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)

    # Parameters
    sequence_length = 10
    num_classes = 2
    vocab_size = 5000
    emb_dim = 128
    filter_sizes = [3, 4, 5]
    num_filters = [100, 100, 100]
    dropout = 0.5

    # Create discriminator model
    discriminator = Discriminator(sequence_length, num_classes, vocab_size, emb_dim, filter_sizes, num_filters, dropout)

    # Real data (batch_size, seq_len)
    real_data = torch.randint(0, vocab_size, (32, sequence_length))
    real_class_labels = torch.randint(0, num_classes, (32,))
    real_labels = torch.ones((32, 1), dtype=torch.float)  # Real data labels (all 1s)

    # Fake data (batch_size, seq_len)
    fake_data = torch.randint(0, vocab_size, (32, sequence_length))
    fake_class_labels = torch.randint(0, num_classes, (32,))
    fake_labels = torch.zeros((32, 1), dtype=torch.float)  # Fake data labels (all 0s)

    # Forward pass for real data
    real_output, real_class_output = discriminator(real_data)
    real_loss = discriminator.compute_loss(real_output, real_labels, real_class_output, real_class_labels)

    # Forward pass for fake data
    fake_output, fake_class_output = discriminator(fake_data)
    fake_loss = discriminator.compute_loss(fake_output, fake_labels, fake_class_output, fake_class_labels)

    # Total loss
    total_loss = 0.5 * (real_loss + fake_loss)

    print(f"Real Loss: {real_loss.item():.4f}")
    print(f"Fake Loss: {fake_loss.item():.4f}")
    print(f"Total Loss: {total_loss.item():.4f}")